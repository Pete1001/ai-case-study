# ai-case-study
Module 1 Challenge by Pete Link

# **A Case of Anthropic; where Claude excels**

## Overview and Origin
* ### **Anthropic**
* Anthropic is a US based AI startup founded in 2021.  Anthropic is mostly known for its family of large language models (LLMs) named Claude.  Claude is part of a set of models, often called GPTs, who's competitors are OpenAI's ChatGPT and Google's Gemini as well as others.
* Anthropic's founders are Italian American siblings Daniela and Dario Amodei.  Since its founding in 2021, the company has received significant funding of almost $7 billion from Google, Amazon, Menlo Ventures, Wisdom Ventures, Ripple Impact Investments and Factorial Funds.
* The idea of Anthropic came from the dissatisfaction with OpenAI's ventures with Microsoft.  Anthropic incorporated in Delaware as a public-benefit corporation (PBC) which requires it to satisfy a balance between public and private trust and governs the company's interests between public and private benefit rather than simply profit.

## Business Activities
* Anthropic's LLMs Claude were developed to outperform other competitors such as OpenAI's ChatGPT.  One of the key features where Claude models outperforms its competitors is the ability to analyze images.
* Anthropicâ€™s advantage in the market is a novel approach called Constitutional AI[^1].  Constitutional AI is a feedback-based learning in two phases.  In the first phase called supervise learning, the model generates responses to prompts based on guiding principles, a so called constitution which consists of 75 points and includes sections from the UN Universal Declaration of Human Rights.  In the second phase, called reinforcement learning, generated responses are compared to compliance with the constitution of guiding principles.
* This unique approach based on guiding principles, gives Claude LLMs (sometimes called GPTs) the ability to respond appropriately to harmful requests without requiring human intervention.

> [^1]: *More information on Constitutional AI can be found here:  https://www.anthropic.com/research/constitutional-ai-harmlessness-from-ai-feedback*

## Landscape
* AI based research firm, based in San Francisco placing safety at the frontier of its products
* Constitutional AI is the company's biggest innovation embedded in its GPT models such as Claude
* Competitors include OpenAI's ChatGPT, Google's Gemini and Microsoft Copilot

## Results
* The business impact of Anthropic's GPTs such as Claude is their commitment to "helpful, harmless and honest".  This approach allows Claude to put human safety guardrails around its models to allow for a more human approach.
* One of the success criteria, that Anthropic used to outperform its competitors is its ability to answer in a more human like way.  This was demonstrated by presenting various ethical dilemmas where the GPT was asked to provide an answer.  The results of these ethical trials were truly stunning wiht a human-like approach.

## Recommendations
1. Anthropic's GPT model is instructed to respond in ways that wouldn't 'offend' a child in its Claude models.  I would thus recommend that the company develops a model for adults where all situations and responses are considered.
2. A true benefit would be a real, life-like scenarios.  For example, one of the examples sited regarding a situation where Claude is asked to make a decision of what to do when it is confronted with a situation where a robber, steals money from a bank and gives it to an orphanage.  In such a situation, Cluse recommended not reporting the robber and discouraging him from committing it again.  This is great in a utopian society; however, in a society full of faulty humans, a commitment to a rule of law is necessary to discourage others from such behaviors.
3. The adult Claude model would simply remove the guard rails placed there as not to offend and add additional Constitutional models that include rule of law, traffic rules, and other rules that govern our society such as The Constitution itself, FAA rules, all law precedents from the court systems, and other human rules.  Such as don't eat rocks, glue, or drink vile fluids!
4. This approach would make the model more realistic and would help avoid situations such as Google's AI recommending using glue on pizza or eating one rock per day.

## Sources:
- [Company overview](https://www.anthropic.com/company)
- [Wikipedia summary](https://en.wikipedia.org/wiki/Anthropic)
- [ChatGPT alternatives](https://zapier.com/blog/chatgpt-alternatives/)
- [Google's failures with AI](https://www.forbes.com/sites/jackkelly/2024/05/31/google-ai-glue-to-pizza-viral-blunders/)
- [Cluade vs ChatGPT](https://www.semrush.com/goodcontent/content-marketing-blog/claude-vs-chatgpt/)
- [Constitutional AI](https://www.anthropic.com/research/constitutional-ai-harmlessness-from-ai-feedback)

